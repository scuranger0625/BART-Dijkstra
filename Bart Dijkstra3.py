import json
import torch
import re
import time
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from tqdm import tqdm  # âœ… åŠ å…¥é€²åº¦æ¢

# âœ… ä¸­æ–‡ BART æ¨¡å‹è¨­å®š
print("ğŸš€ æ­£åœ¨è¼‰å…¥ BART æ¨¡å‹...", flush=True)
model_name = "fnlp/bart-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
device = torch.device("cpu")
model = model.to(device)
print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼", flush=True)

# è®€å–åœç”¨è©ï¼ˆæ¯è¡Œä¸€å€‹è©ï¼‰
stopwords_path = r"C:\Users\Leon\Desktop\Topic Model\æœ¬åœ°ç’°å¢ƒ\clearstopwords2.txt"
with open(stopwords_path, 'r', encoding='utf-8') as f:
    stopwords = set(line.strip() for line in f if line.strip())

# è®€å– LDA ä¸»é¡Œçš„é—œéµè©ï¼ˆlda_top_words.leiden.txtï¼‰
lda_keywords_path = r"C:\Users\Leon\Desktop\Topic Model\æœ¬åœ°ç’°å¢ƒ\lda_top_words.leiden.txt"
with open(lda_keywords_path, 'r', encoding='utf-8') as f:
    lda_keywords_raw = f.read().strip().split("\n")

# åˆ†å‰²æˆå„å€‹ä¸»é¡Œçš„è©å½™ï¼ˆéæ¿¾1å­—è©ï¼‰
lda_keywords = {}
for line in lda_keywords_raw:
    topic_id, words = line.split(":")
    words = words.strip()[1:-1].split(",")
    filtered_words = [word.strip().replace('"', '') for word in words if len(word.strip().replace('"', '')) >= 2]
    lda_keywords[int(topic_id.replace('Topic', '').strip())] = filtered_words

# åœç”¨è©
combined_filter_words = stopwords

# ç§»é™¤åœç”¨è©
def remove_filtered_phrases(text, filter_words):
    for word in filter_words:
        text = text.replace(word, "")
    return text

# æ¸…æ´—æ–‡æœ¬
def clean_text_content(text, filter_words):
    text = remove_filtered_phrases(text, filter_words)
    text = re.sub(r"[^\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿ]", "", text)
    return text

# å»ºç«‹èªæ„åœ–ä¸¦æ‰¾æœ€çŸ­ä¸»é¡Œè·¯å¾‘ Dijkstraæ¼”ç®—æ³•
def get_dijkstra_topic_path(words):
    if len(words) < 2:
        return words
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(words)
    sim_matrix = cosine_similarity(tfidf_matrix)
    G = nx.Graph()
    for i, w1 in enumerate(words):
        for j, w2 in enumerate(words):
            if i != j:
                G.add_edge(w1, w2, weight=1 - sim_matrix[i, j])
    best_path = []
    min_cost = float("inf")
    for start in words:
        for end in words:
            if start != end and nx.has_path(G, start, end):
                try:
                    path = nx.dijkstra_path(G, start, end, weight='weight')
                    cost = nx.path_weight(G, path, weight='weight')
                    if cost < min_cost:
                        best_path = path
                        min_cost = cost
                except:
                    continue
    return best_path if best_path else words[:3]

# åŠ å…¥ LDA é—œéµè©åˆ°æ–‡æœ¬é–‹é ­
def add_lda_keywords_to_text(text, lda_keywords, topic_id):
    lda_keywords_text = "ã€".join(lda_keywords.get(topic_id, []))
    return lda_keywords_text + "ã€‚" + text

# è®€å–åŸå§‹ JSON
file_path = r"C:\Users\Leon\Desktop\Topic Model\æœ¬åœ°ç’°å¢ƒ\topictexts_from_gamma.json"
with open(file_path, 'r', encoding='utf-8') as f:
    topic_texts = json.load(f)

# åˆ¤æ–·çµæ§‹
entries = topic_texts.items() if isinstance(topic_texts, dict) else topic_texts
print(f"âœ… è¼‰å…¥è³‡æ–™ç­†æ•¸ï¼š{len(entries)}", flush=True)

# è¼¸å‡ºè·¯å¾‘
output_path = r"C:\Users\Leon\Desktop\Topic Model\æœ¬åœ°ç’°å¢ƒ\bart dijkstraä¸­æ–‡æ‘˜è¦çµæœ.json"
summaries = []

print("ğŸš€ é–‹å§‹è™•ç†è³‡æ–™...", flush=True)
start_time = time.time()  # â±ï¸ è¨ˆæ™‚é–‹å§‹

# é€ç­†è™•ç†æ–‡æœ¬ï¼ˆåŠ ä¸Š tqdm é€²åº¦æ¢ï¼‰
for idx, item in tqdm(list(enumerate(entries)), desc="è™•ç†é€²åº¦"):
    text = item[1] if isinstance(item, tuple) else item.get("text", "")
    if isinstance(text, list):
        text = " ".join(str(x) for x in text)
    elif not isinstance(text, str):
        continue

    topic_id = (idx % 30) + 1
    text = add_lda_keywords_to_text(text, lda_keywords, topic_id)
    text = clean_text_content(text, combined_filter_words)
    if not text.strip():
        continue

    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True).to(device)
    summary_ids = model.generate(inputs["input_ids"], max_length=150, num_beams=6, temperature=1.0, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    topic_words = lda_keywords.get(topic_id, [])
    best_path = get_dijkstra_topic_path(topic_words)
    semantic_hint = "ã€".join(best_path)
    title_prompt = f"ä»¥ä¸‹ç‚ºä¸€ç¯‡æ–°èæ‘˜è¦ï¼š{summary}ã€‚è«‹æ ¹æ“šå…§å®¹å¹«é€™ç¯‡æ–‡ç« å‘½åä¸€å€‹ä¸»é¡Œã€‚"

    title_input = tokenizer(title_prompt, return_tensors="pt", max_length=128, truncation=True).to(device)
    title_ids = model.generate(title_input["input_ids"], max_length=30, num_beams=4, temperature=1.0, early_stopping=True) # max_lengthåœ¨é€™è£¡æ”¹åƒæ•¸ 
    topic_title = tokenizer.decode(title_ids[0], skip_special_tokens=True)

    summaries.append({
        "id": topic_id,
        "summary": summary,
        "topic_title": topic_title
    })

end_time = time.time()  # â±ï¸ è¨ˆæ™‚çµæŸ

# å„²å­˜ç‚º JSON
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(summaries, f, ensure_ascii=False, indent=2)

print("ğŸ‰ ä¸­æ–‡æ‘˜è¦èˆ‡ä¸»é¡Œå‘½åå®Œæˆï¼ˆå« Dijkstra å¼·åŒ–ï¼‰ï¼", flush=True)
print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“ï¼š{end_time - start_time:.2f} ç§’", flush=True)
